{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "So we are about to extract features from the dataset sound using MFCC, put it in a json file,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset related packages\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import json\n",
    "\n",
    "sample_length = 22050*1 # sampling rate\n",
    "sample_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"MFCCs\": [],\n",
    "        \"files\": []\n",
    "    }\n",
    "    count=0\n",
    "\n",
    "    # loop through all sub-dirs\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        # ensure we're at sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save label (i.e., sub-folder name) in the mapping\n",
    "            label = dirpath.split(\"/\")[-1]\n",
    "            data[\"mapping\"].append(label)\n",
    "            print(\"\\nProcessing: '{}'\".format(label))\n",
    "\n",
    "            # process all audio files in sub-dir and store MFCCs\n",
    "            for f in filenames:\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "\n",
    "                # load audio file and slice it to ensure length consistency among different files\n",
    "                signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "                # drop audio files with less than pre-decided number of samples\n",
    "                if len(signal) >= sample_length:\n",
    "\n",
    "                    # ensure consistency of the length of the signal\n",
    "                    signal = signal[:sample_length]\n",
    "\n",
    "                    # extract MFCCs\n",
    "                    MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                                    hop_length=hop_length)\n",
    "\n",
    "                    # store data for analysed track\n",
    "                    count=count+1\n",
    "                    data[\"MFCCs\"].append(MFCCs.T.tolist())\n",
    "                    data[\"labels\"].append(i-1)\n",
    "                    data[\"files\"].append(file_path)\n",
    "                    print(str(count)+\" {}: {}\".format(file_path, i-1))\n",
    "\n",
    "    # save data in json file\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'domestic'\n",
      "1 dataset/mixed/domestic\\00 - Dataset - Amanda2.wav: 0\n",
      "2 dataset/mixed/domestic\\00 - Dataset - WhatsApp Audio 2021-05-23 at 07.43.49 (1).wav: 0\n",
      "3 dataset/mixed/domestic\\00 - domestic - audio 101.wav: 0\n",
      "4 dataset/mixed/domestic\\00 - domestic - audio 103.wav: 0\n",
      "5 dataset/mixed/domestic\\00 - domestic - audio 104.wav: 0\n",
      "6 dataset/mixed/domestic\\00 - domestic - audio 105.wav: 0\n",
      "7 dataset/mixed/domestic\\00 - domestic - audio 106.wav: 0\n",
      "8 dataset/mixed/domestic\\00 - domestic - audio 107.wav: 0\n",
      "9 dataset/mixed/domestic\\00 - domestic - audio 109.wav: 0\n",
      "10 dataset/mixed/domestic\\00 - domestic - audio 110.wav: 0\n",
      "11 dataset/mixed/domestic\\00 - domestic - audio 111.wav: 0\n",
      "12 dataset/mixed/domestic\\00 - domestic - audio 112.wav: 0\n",
      "13 dataset/mixed/domestic\\00 - domestic - audio 118.wav: 0\n",
      "14 dataset/mixed/domestic\\00 - domestic - audio 120.wav: 0\n",
      "15 dataset/mixed/domestic\\00 - domestic - audio 19.wav: 0\n",
      "16 dataset/mixed/domestic\\00 - domestic - audio 21.wav: 0\n",
      "17 dataset/mixed/domestic\\00 - domestic - audio 23.wav: 0\n",
      "18 dataset/mixed/domestic\\00 - domestic - audio 24.wav: 0\n",
      "19 dataset/mixed/domestic\\00 - domestic - audio 25.wav: 0\n",
      "20 dataset/mixed/domestic\\00 - domestic - audio 26.wav: 0\n",
      "21 dataset/mixed/domestic\\00 - domestic - audio 27.wav: 0\n",
      "22 dataset/mixed/domestic\\00 - domestic - audio 29.wav: 0\n",
      "23 dataset/mixed/domestic\\00 - domestic - audio 32.wav: 0\n",
      "24 dataset/mixed/domestic\\00 - domestic - audio 38.wav: 0\n",
      "25 dataset/mixed/domestic\\00 - domestic - audio 59.wav: 0\n",
      "26 dataset/mixed/domestic\\00 - domestic - audio 61.wav: 0\n",
      "27 dataset/mixed/domestic\\00 - domestic - audio 63.wav: 0\n",
      "28 dataset/mixed/domestic\\00 - domestic - audio 64.wav: 0\n",
      "29 dataset/mixed/domestic\\00 - domestic - audio 65.wav: 0\n",
      "30 dataset/mixed/domestic\\00 - domestic - audio 66.wav: 0\n",
      "31 dataset/mixed/domestic\\00 - domestic - audio 67.wav: 0\n",
      "32 dataset/mixed/domestic\\00 - domestic - audio 69.wav: 0\n",
      "33 dataset/mixed/domestic\\00 - domestic - audio 71.wav: 0\n",
      "34 dataset/mixed/domestic\\00 - domestic - audio 78.wav: 0\n",
      "35 dataset/mixed/domestic\\00 - domestic - audio 80.wav: 0\n",
      "36 dataset/mixed/domestic\\00 - domestic - audio 84.wav: 0\n",
      "37 dataset/mixed/domestic\\00 - domestic - audio 85.wav: 0\n",
      "38 dataset/mixed/domestic\\00 - domestic - audio 99.wav: 0\n",
      "39 dataset/mixed/domestic\\audio 101.ogg: 0\n",
      "40 dataset/mixed/domestic\\audio 103.ogg: 0\n",
      "41 dataset/mixed/domestic\\audio 104.ogg: 0\n",
      "42 dataset/mixed/domestic\\audio 105.ogg: 0\n",
      "43 dataset/mixed/domestic\\audio 106.ogg: 0\n",
      "44 dataset/mixed/domestic\\audio 107.ogg: 0\n",
      "45 dataset/mixed/domestic\\audio 109.ogg: 0\n",
      "46 dataset/mixed/domestic\\audio 110.ogg: 0\n",
      "47 dataset/mixed/domestic\\audio 111.ogg: 0\n",
      "48 dataset/mixed/domestic\\audio 112.ogg: 0\n",
      "49 dataset/mixed/domestic\\audio 118.ogg: 0\n",
      "50 dataset/mixed/domestic\\audio 120.ogg: 0\n",
      "51 dataset/mixed/domestic\\audio 19.ogg: 0\n",
      "52 dataset/mixed/domestic\\audio 21.ogg: 0\n",
      "53 dataset/mixed/domestic\\audio 23.ogg: 0\n",
      "54 dataset/mixed/domestic\\audio 24.ogg: 0\n",
      "55 dataset/mixed/domestic\\audio 25.ogg: 0\n",
      "56 dataset/mixed/domestic\\audio 26.ogg: 0\n",
      "57 dataset/mixed/domestic\\audio 27.ogg: 0\n",
      "58 dataset/mixed/domestic\\audio 29.ogg: 0\n",
      "59 dataset/mixed/domestic\\audio 32.ogg: 0\n",
      "60 dataset/mixed/domestic\\audio 38.ogg: 0\n",
      "61 dataset/mixed/domestic\\audio 59.ogg: 0\n",
      "62 dataset/mixed/domestic\\audio 61.ogg: 0\n",
      "63 dataset/mixed/domestic\\audio 63.ogg: 0\n",
      "64 dataset/mixed/domestic\\audio 64.ogg: 0\n",
      "65 dataset/mixed/domestic\\audio 65.ogg: 0\n",
      "66 dataset/mixed/domestic\\audio 66.ogg: 0\n",
      "67 dataset/mixed/domestic\\audio 67.ogg: 0\n",
      "68 dataset/mixed/domestic\\audio 69.ogg: 0\n",
      "69 dataset/mixed/domestic\\audio 71.ogg: 0\n",
      "70 dataset/mixed/domestic\\audio 78.ogg: 0\n",
      "71 dataset/mixed/domestic\\audio 80.ogg: 0\n",
      "72 dataset/mixed/domestic\\audio 84.ogg: 0\n",
      "73 dataset/mixed/domestic\\audio 85.ogg: 0\n",
      "74 dataset/mixed/domestic\\audio 99.ogg: 0\n",
      "\n",
      "Processing: 'physical'\n",
      "75 dataset/mixed/physical\\00 - Dataset - Amanda3.wav: 1\n",
      "76 dataset/mixed/physical\\00 - Dataset - Amanda4.wav: 1\n",
      "77 dataset/mixed/physical\\00 - Dataset - Amanda5.wav: 1\n",
      "78 dataset/mixed/physical\\00 - Dataset - voice 433029.wav: 1\n",
      "79 dataset/mixed/physical\\00 - Dataset - voice 433030.wav: 1\n",
      "80 dataset/mixed/physical\\00 - Dataset - voice 433077.wav: 1\n",
      "81 dataset/mixed/physical\\00 - Dataset - voice 433079.wav: 1\n",
      "82 dataset/mixed/physical\\00 - Dataset - voice 433081.wav: 1\n",
      "83 dataset/mixed/physical\\00 - Dataset - voice 433082.wav: 1\n",
      "84 dataset/mixed/physical\\00 - Dataset - voice 433085.wav: 1\n",
      "85 dataset/mixed/physical\\00 - Dataset - voice 433087.wav: 1\n",
      "86 dataset/mixed/physical\\00 - Dataset - WhatsApp Audio 2021-05-23 at 22.35.53 (2).wav: 1\n",
      "87 dataset/mixed/physical\\00 - physical - audio 102.wav: 1\n",
      "88 dataset/mixed/physical\\00 - physical - audio 108.wav: 1\n",
      "89 dataset/mixed/physical\\00 - physical - audio 113.wav: 1\n",
      "90 dataset/mixed/physical\\00 - physical - audio 115.wav: 1\n",
      "91 dataset/mixed/physical\\00 - physical - audio 116.wav: 1\n",
      "92 dataset/mixed/physical\\00 - physical - audio 117.wav: 1\n",
      "93 dataset/mixed/physical\\00 - physical - audio 13.wav: 1\n",
      "94 dataset/mixed/physical\\00 - physical - audio 15.wav: 1\n",
      "95 dataset/mixed/physical\\00 - physical - audio 16.wav: 1\n",
      "96 dataset/mixed/physical\\00 - physical - audio 28.wav: 1\n",
      "97 dataset/mixed/physical\\00 - physical - audio 3.wav: 1\n",
      "98 dataset/mixed/physical\\00 - physical - audio 33.wav: 1\n",
      "99 dataset/mixed/physical\\00 - physical - audio 35.wav: 1\n",
      "100 dataset/mixed/physical\\00 - physical - audio 37.wav: 1\n",
      "101 dataset/mixed/physical\\00 - physical - audio 4.wav: 1\n",
      "102 dataset/mixed/physical\\00 - physical - audio 45.wav: 1\n",
      "103 dataset/mixed/physical\\00 - physical - audio 5.wav: 1\n",
      "104 dataset/mixed/physical\\00 - physical - audio 51.wav: 1\n",
      "105 dataset/mixed/physical\\00 - physical - audio 52.wav: 1\n",
      "106 dataset/mixed/physical\\00 - physical - audio 56.wav: 1\n",
      "107 dataset/mixed/physical\\00 - physical - audio 6.wav: 1\n",
      "108 dataset/mixed/physical\\00 - physical - audio 68.wav: 1\n",
      "109 dataset/mixed/physical\\00 - physical - audio 7.wav: 1\n",
      "110 dataset/mixed/physical\\00 - physical - audio 73.wav: 1\n",
      "111 dataset/mixed/physical\\00 - physical - audio 75.wav: 1\n",
      "112 dataset/mixed/physical\\00 - physical - audio 76.wav: 1\n",
      "113 dataset/mixed/physical\\00 - physical - audio 77.wav: 1\n",
      "114 dataset/mixed/physical\\00 - physical - audio 84.wav: 1\n",
      "115 dataset/mixed/physical\\00 - physical - audio 85.wav: 1\n",
      "116 dataset/mixed/physical\\00 - physical - audio 87.wav: 1\n",
      "117 dataset/mixed/physical\\00 - physical - audio 89.wav: 1\n",
      "118 dataset/mixed/physical\\00 - physical - audio 91.wav: 1\n",
      "119 dataset/mixed/physical\\00 - physical - audio 92.wav: 1\n",
      "120 dataset/mixed/physical\\00 - physical - audio 96.wav: 1\n",
      "121 dataset/mixed/physical\\00 - physical - audio 98.wav: 1\n",
      "122 dataset/mixed/physical\\audio 102.ogg: 1\n",
      "123 dataset/mixed/physical\\audio 108.ogg: 1\n",
      "124 dataset/mixed/physical\\audio 113.ogg: 1\n",
      "125 dataset/mixed/physical\\audio 115.ogg: 1\n",
      "126 dataset/mixed/physical\\audio 116.ogg: 1\n",
      "127 dataset/mixed/physical\\audio 117.ogg: 1\n",
      "128 dataset/mixed/physical\\audio 13.ogg: 1\n",
      "129 dataset/mixed/physical\\audio 15.ogg: 1\n",
      "130 dataset/mixed/physical\\audio 16.ogg: 1\n",
      "131 dataset/mixed/physical\\audio 28.ogg: 1\n",
      "132 dataset/mixed/physical\\audio 3.ogg: 1\n",
      "133 dataset/mixed/physical\\audio 33.ogg: 1\n",
      "134 dataset/mixed/physical\\audio 35.ogg: 1\n",
      "135 dataset/mixed/physical\\audio 37.ogg: 1\n",
      "136 dataset/mixed/physical\\audio 4.ogg: 1\n",
      "137 dataset/mixed/physical\\audio 45.ogg: 1\n",
      "138 dataset/mixed/physical\\audio 5.ogg: 1\n",
      "139 dataset/mixed/physical\\audio 51.ogg: 1\n",
      "140 dataset/mixed/physical\\audio 52.ogg: 1\n",
      "141 dataset/mixed/physical\\audio 56.ogg: 1\n",
      "142 dataset/mixed/physical\\audio 6.ogg: 1\n",
      "143 dataset/mixed/physical\\audio 68.ogg: 1\n",
      "144 dataset/mixed/physical\\audio 7.ogg: 1\n",
      "145 dataset/mixed/physical\\audio 73.ogg: 1\n",
      "146 dataset/mixed/physical\\audio 75.ogg: 1\n",
      "147 dataset/mixed/physical\\audio 76.ogg: 1\n",
      "148 dataset/mixed/physical\\audio 77.ogg: 1\n",
      "149 dataset/mixed/physical\\audio 84.ogg: 1\n",
      "150 dataset/mixed/physical\\audio 85.ogg: 1\n",
      "151 dataset/mixed/physical\\audio 87.ogg: 1\n",
      "152 dataset/mixed/physical\\audio 89.ogg: 1\n",
      "153 dataset/mixed/physical\\audio 91.ogg: 1\n",
      "154 dataset/mixed/physical\\audio 92.ogg: 1\n",
      "155 dataset/mixed/physical\\audio 96.ogg: 1\n",
      "156 dataset/mixed/physical\\audio 98.ogg: 1\n",
      "\n",
      "Processing: 'sexual'\n",
      "157 dataset/mixed/sexual\\00 - Dataset - Amanda1.wav: 2\n",
      "158 dataset/mixed/sexual\\00 - Dataset - voice 433026.wav: 2\n",
      "159 dataset/mixed/sexual\\00 - Dataset - voice 433046.wav: 2\n",
      "160 dataset/mixed/sexual\\00 - Dataset - voice 433084.wav: 2\n",
      "161 dataset/mixed/sexual\\00 - Dataset - WhatsApp Ptt 2021-05-22 at 21.18.20.wav: 2\n",
      "162 dataset/mixed/sexual\\00 - Dataset - WhatsApp Ptt 2021-05-22 at 21.18.45.wav: 2\n",
      "163 dataset/mixed/sexual\\00 - Dataset - WhatsApp Ptt 2021-05-22 at 21.19.06.wav: 2\n",
      "164 dataset/mixed/sexual\\00 - Dataset - WhatsApp Ptt 2021-05-22 at 21.19.25.wav: 2\n",
      "165 dataset/mixed/sexual\\00 - Dataset - WhatsApp Ptt 2021-05-22 at 21.19.36.wav: 2\n",
      "166 dataset/mixed/sexual\\00 - sexual - audio 1.wav: 2\n",
      "167 dataset/mixed/sexual\\00 - sexual - audio 102.wav: 2\n",
      "168 dataset/mixed/sexual\\00 - sexual - audio 11.wav: 2\n",
      "169 dataset/mixed/sexual\\00 - sexual - audio 110.wav: 2\n",
      "170 dataset/mixed/sexual\\00 - sexual - audio 112.wav: 2\n",
      "171 dataset/mixed/sexual\\00 - sexual - audio 114.wav: 2\n",
      "172 dataset/mixed/sexual\\00 - sexual - audio 115.wav: 2\n",
      "173 dataset/mixed/sexual\\00 - sexual - audio 116.wav: 2\n",
      "174 dataset/mixed/sexual\\00 - sexual - audio 12.wav: 2\n",
      "175 dataset/mixed/sexual\\00 - sexual - audio 120.wav: 2\n",
      "176 dataset/mixed/sexual\\00 - sexual - audio 14.wav: 2\n",
      "177 dataset/mixed/sexual\\00 - sexual - audio 16.wav: 2\n",
      "178 dataset/mixed/sexual\\00 - sexual - audio 17.wav: 2\n",
      "179 dataset/mixed/sexual\\00 - sexual - audio 18.wav: 2\n",
      "180 dataset/mixed/sexual\\00 - sexual - audio 22.wav: 2\n",
      "181 dataset/mixed/sexual\\00 - sexual - audio 30.wav: 2\n",
      "182 dataset/mixed/sexual\\00 - sexual - audio 31.wav: 2\n",
      "183 dataset/mixed/sexual\\00 - sexual - audio 36.wav: 2\n",
      "184 dataset/mixed/sexual\\00 - sexual - audio 40.wav: 2\n",
      "185 dataset/mixed/sexual\\00 - sexual - audio 42.wav: 2\n",
      "186 dataset/mixed/sexual\\00 - sexual - audio 44.wav: 2\n",
      "187 dataset/mixed/sexual\\00 - sexual - audio 47.wav: 2\n",
      "188 dataset/mixed/sexual\\00 - sexual - audio 49.wav: 2\n",
      "189 dataset/mixed/sexual\\00 - sexual - audio 54.wav: 2\n",
      "190 dataset/mixed/sexual\\00 - sexual - audio 55.wav: 2\n",
      "191 dataset/mixed/sexual\\00 - sexual - audio 58.wav: 2\n",
      "192 dataset/mixed/sexual\\00 - sexual - audio 62.wav: 2\n",
      "193 dataset/mixed/sexual\\00 - sexual - audio 7.wav: 2\n",
      "194 dataset/mixed/sexual\\00 - sexual - audio 70.wav: 2\n",
      "195 dataset/mixed/sexual\\00 - sexual - audio 72.wav: 2\n",
      "196 dataset/mixed/sexual\\00 - sexual - audio 80.wav: 2\n",
      "197 dataset/mixed/sexual\\00 - sexual - audio 82.wav: 2\n",
      "198 dataset/mixed/sexual\\00 - sexual - audio 83.wav: 2\n",
      "199 dataset/mixed/sexual\\00 - sexual - audio 87.wav: 2\n",
      "200 dataset/mixed/sexual\\00 - sexual - audio 89.wav: 2\n",
      "201 dataset/mixed/sexual\\00 - sexual - audio 94.wav: 2\n",
      "202 dataset/mixed/sexual\\00 - sexual - audio 95.wav: 2\n",
      "203 dataset/mixed/sexual\\00 - sexual - audio 96.wav: 2\n",
      "204 dataset/mixed/sexual\\00 - sexual - audio 97.wav: 2\n",
      "205 dataset/mixed/sexual\\00 - sexual - audio 98.wav: 2\n",
      "206 dataset/mixed/sexual\\audio 1.ogg: 2\n",
      "207 dataset/mixed/sexual\\audio 102.ogg: 2\n",
      "208 dataset/mixed/sexual\\audio 11.ogg: 2\n",
      "209 dataset/mixed/sexual\\audio 110.ogg: 2\n",
      "210 dataset/mixed/sexual\\audio 112.ogg: 2\n",
      "211 dataset/mixed/sexual\\audio 114.ogg: 2\n",
      "212 dataset/mixed/sexual\\audio 115.ogg: 2\n",
      "213 dataset/mixed/sexual\\audio 116.ogg: 2\n",
      "214 dataset/mixed/sexual\\audio 12.ogg: 2\n",
      "215 dataset/mixed/sexual\\audio 120.ogg: 2\n",
      "216 dataset/mixed/sexual\\audio 14.ogg: 2\n",
      "217 dataset/mixed/sexual\\audio 16.ogg: 2\n",
      "218 dataset/mixed/sexual\\audio 17.ogg: 2\n",
      "219 dataset/mixed/sexual\\audio 18.ogg: 2\n",
      "220 dataset/mixed/sexual\\audio 22.ogg: 2\n",
      "221 dataset/mixed/sexual\\audio 30.ogg: 2\n",
      "222 dataset/mixed/sexual\\audio 31.ogg: 2\n",
      "223 dataset/mixed/sexual\\audio 36.ogg: 2\n",
      "224 dataset/mixed/sexual\\audio 40.ogg: 2\n",
      "225 dataset/mixed/sexual\\audio 42.ogg: 2\n",
      "226 dataset/mixed/sexual\\audio 44.ogg: 2\n",
      "227 dataset/mixed/sexual\\audio 47.ogg: 2\n",
      "228 dataset/mixed/sexual\\audio 49.ogg: 2\n",
      "229 dataset/mixed/sexual\\audio 54.ogg: 2\n",
      "230 dataset/mixed/sexual\\audio 55.ogg: 2\n",
      "231 dataset/mixed/sexual\\audio 58.ogg: 2\n",
      "232 dataset/mixed/sexual\\audio 62.ogg: 2\n",
      "233 dataset/mixed/sexual\\audio 7.ogg: 2\n",
      "234 dataset/mixed/sexual\\audio 70.ogg: 2\n",
      "235 dataset/mixed/sexual\\audio 72.ogg: 2\n",
      "236 dataset/mixed/sexual\\audio 80.ogg: 2\n",
      "237 dataset/mixed/sexual\\audio 82.ogg: 2\n",
      "238 dataset/mixed/sexual\\audio 83.ogg: 2\n",
      "239 dataset/mixed/sexual\\audio 87.ogg: 2\n",
      "240 dataset/mixed/sexual\\audio 89.ogg: 2\n",
      "241 dataset/mixed/sexual\\audio 94.ogg: 2\n",
      "242 dataset/mixed/sexual\\audio 95.ogg: 2\n",
      "243 dataset/mixed/sexual\\audio 96.ogg: 2\n",
      "244 dataset/mixed/sexual\\audio 97.ogg: 2\n",
      "245 dataset/mixed/sexual\\audio 98.ogg: 2\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset('dataset/mixed/', 'mixedformat.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call json and train the model\n",
    "\n",
    "call the json, put the features into the neural model, fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OrderedDict' from 'typing' (c:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3432\\1939206707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    417\u001b[0m \"\"\"\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"API defining checkpoint.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheckpoint_view\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint_view.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrackable_view\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint\\trackable_view.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\trackable\\converter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\saved_model_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0masset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\trackable\\resource.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrackable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Config Options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_dynamic_variable_creation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_functions_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions_run_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlift_to_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_spec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfunction_spec_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonomorphic_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtracing_compiler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_spec.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphism\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfunction_type_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolymorphic_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OrderedDict' from 'typing' (c:\\Users\\arnab\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)"
     ]
    }
   ],
   "source": [
    "#import training related packages\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_visualizer import visualizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "DATA_PATH = \"mixedformat.json\"\n",
    "SAVED_MODEL_PATH = \"training_mix_reajusted_input.h5\"\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path, test_size=0.1, validation_size=0.1):\n",
    "    # load dataset\n",
    "    X, y = load_data(data_path)\n",
    "\n",
    "    # create train, validation, test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    # add an axis to nd array\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"MFCCs\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    print(\"Training sets loaded!\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, loss=\"sparse_categorical_crossentropy\", learning_rate=0.0001):\n",
    "    \n",
    "    # build network architecture using convolutional layers\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.00001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "    tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.00001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
    "\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.00001)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n",
    "\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    # flatten output and feed into dense layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    # softmax output layer\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss=loss,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # print model parameters on console\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):\n",
    "    \n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0.001, patience=patience)\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_validation, y_validation),\n",
    "                        callbacks=[earlystop_callback])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    plt.figure(figsize=(5,10))\n",
    "\n",
    "    # create accuracy subplot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "    axs[0].plot(history.history['val_accuracy'], label=\"val_accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy evaluation\")\n",
    "\n",
    "    # create loss subplot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"loss\")\n",
    "    axs[1].plot(history.history['val_loss'], label=\"val_loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss evaluation\")\n",
    "\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "model = build_model(input_shape, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction \n",
    "Let's try this guy out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the input sample function\n",
    "def preprocess(file_path, num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "        \"\"\"Extract MFCCs from audio file.\n",
    "        :param file_path (str): Path of audio file\n",
    "        :param num_mfcc (int): # of coefficients to extract\n",
    "        :param n_fft (int): Interval we consider to apply STFT. Measured in # of samples\n",
    "        :param hop_length (int): Sliding window for STFT. Measured in # of samples\n",
    "        :return MFCCs (ndarray): 2-dim array with MFCC data of shape (# time steps, # coefficients)\n",
    "        \"\"\"\n",
    "\n",
    "        # load audio file\n",
    "        signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "        if len(signal) >= sample_length:\n",
    "            # ensure consistency of the length of the signal\n",
    "            signal = signal[:sample_length]\n",
    "\n",
    "            # extract MFCCs\n",
    "            MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                         hop_length=hop_length)\n",
    "        return MFCCs.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded=tf.keras.models.load_model(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file_path):\n",
    "       \n",
    "        # extract MFCC\n",
    "        MFCCs = preprocess(file_path)\n",
    "\n",
    "        # we need a 4-dim array to feed to the model for prediction: (# samples, # time steps, # coefficients, 1)\n",
    "        MFCCs = MFCCs[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "        # get the predicted label\n",
    "        predictions = loaded.predict(MFCCs)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=predict(\"./dataset/mixkit-arcade-game-explosion-2759.wav\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Violence Prediction: \\n\")\n",
    "print(\"Dog Barking: \"+str(res[0,0]*100)+\"%\")\n",
    "print(\"Demoestic Violence: \"+str(res[0,1]*100)+\"%\")\n",
    "print(\"Explosion: \"+str(res[0,2]*100)+\"%\")\n",
    "print(\"Gun Shot: \"+str(res[0,3]*100)+\"%\")\n",
    "print(\"Lightning: \"+str(res[0,4]*100)+\"%\")\n",
    "print(\"Physical Violence: \"+str(res[0,5]*100)+\"%\")\n",
    "print(\"Sexual Violence: \"+str(res[0,6]*100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
